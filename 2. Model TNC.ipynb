{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba0a27b",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Piotr Durniat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfddba",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d10a9b",
   "metadata": {},
   "source": [
    "# 2. Model TNC\n",
    "\n",
    "Przedstawimy teraz zasadę działania modelu TNC, zaimplementujemy jego najistotniejsze elementy oraz dokonamy ewaluacji modelu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fafa4f",
   "metadata": {},
   "source": [
    "## 2.1. Idea modelu TNC\n",
    "\n",
    "Dla ciągu czasowego $X \\in \\mathbb{R}^{D \\times T}$ możemy określić okno przesuwne $X_{[t - \\frac{\\delta}{2}, t + \\frac{\\delta}{2}]}$ o długości $\\delta$ wycentrowane wokół chwili $t$, które zawiera pomiary wszystkich cech w przedziale czasu $[t - \\frac{\\delta}{2}, t + \\frac{\\delta}{2}]$. Dla ułatwienia zapisu okno to będzie oznaczane jako $W_t$.\n",
    "\n",
    "Celem metody uczenia reprezentacji będzie uzyskanie wektorowej reprezentacji dowolnego okna $W_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5da4f8",
   "metadata": {},
   "source": [
    "### Temporalne sąsiedztwo\n",
    "\n",
    "Dla okna $W_t$ zdefiniujmy jego temporalne sąsiedztwo $N_t$ jako zbiór wszystkich okien wycentrowanych w $t^*$, gdzie wartość ta jest próbkowana z rozkładu normalnego $t^* \\sim \\mathcal{N}(t, \\eta \\cdot \\delta)$. Parametr $\\eta$ definiuje szerokość / zakres sąsiedztwa temporalnego.\n",
    "\n",
    "Wybór parametru $\\eta$ może być oparty o wiedzę ekspercką, jednak w publikacji autorzy zaproponowali zastosowanie testu statystycznego **Augmented Dickey-Fuller (ADF)**, który będzie sprawdzać stacjonarność rozkładu próbek w sąsiedztwie temporalnym. Algorytm wyboru parametru zakłada, że rozpoczynamy od niewielkiej wartości, po czym iteracyjnie zwiększamy parametr $\\eta$, powodując poszerzenie się temporalnego sąsiedztwa. Zwiększanie wartości następuje do momentu aż test statystyczny nie będzie w stanie odrzucić hipotezy zerowej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22acd751",
   "metadata": {},
   "source": [
    "### \"Uczenie kontrastowe\"\n",
    "\n",
    "Okna w sąsiedztwie temporalnym możemy uznać jako podobne do obecnie rozważanego okna ciągu czasowego. Pozostaje zatem pytanie jak uzyskać przykłady negatywne, tak abyśmy mogli zastosować koncepcję uczenia kontrastowego.\n",
    "\n",
    "Moglibyśmy założyć, że wszystkie próbki (okna) poza temporalnym sąsiedztwem są przykładami negatywnymi. Może się jednak tak zdarzyć, że nawet bardzo odległe okno, jest w istocie podobne do obecnego (ma taką samą dynamikę zmian / pochodzi z tego samego rozkładu). W takiej sytuacji uczenie modelu nie byłoby efektywne. Autorzy proponują użycie podejścia **Positive-Unlabeled (PU) learning**, w którym mamy próbki bazowe, próbki pozytywne i próbki nieoznaczone. W naszym przykładzie próbkami nieoznaczonymi będą próbki spoza sąsiedztwa temporalnego.\n",
    "\n",
    "Do próbek nieoznaczonych są przypisywane wagi $w$ (tutaj: hiperparametr metody), a każda próbka nieoznaczona jest traktowana jako połączenie próbki pozytywnej z wagą $w$ oraz próbki negatywnej z wagą $1 - w$. Zobaczymy to dalej w funkcji kosztu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7dbc7",
   "metadata": {},
   "source": [
    "### Koder i dyskryminator\n",
    "\n",
    "Model TNC będziemy uczyć za pomocą funkcji kosztu, która pozwala odróżniać reprezentacje próbek z tego samego sąsiedztwa temporalnego od próbek spoza sąsiedztwa.\n",
    "\n",
    "Pierwszym elementem modelu TNC jest **koder** $\\text{Enc}(W_t)$, które przekształca okno $W_t \\in \\mathbb{R}^{D \\times \\delta}$ w wektor reprezentacji $Z_t \\in \\mathbb{R}^M$.\n",
    "\n",
    "Drugim elementem jest **dyskryminator** $\\mathcal{D}(Z_t, Z)$ pozwalający aproksymować prawdopodobieństwo tego, że $Z$ jest reprezentacją okna w sąsiedztwie $N_t$. Innymi słowy, dla dwóch wektorów reprezentacji zwraca pradopodobieństwo tego, że te wektory (okna) należą do tego samego sąsiedztwa temporalnego.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ea895",
   "metadata": {},
   "source": [
    "Przeanalizujmy rysunek przedstawiający zasadę działania modelu TNC:\n",
    "\n",
    "![](./assets/tnc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeea74c",
   "metadata": {},
   "source": [
    "### Funkcja kosztu\n",
    "\n",
    "W najlepszym przypadku dyskryminator powinien zwracać wartości bliskie 1 dla reprezentacji z tego samego temporalnego sąsiedztwa oraz wartość 0 w przeciwnym przypadku. Funkcja kosztu jest zdefiniowana następująco:\n",
    "\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{W_t\\sim X}\\left[ \\mathbb{E}_{W_l \\sim N_t}[\\log \\mathcal{D}(Z_t,Z_l)] +\\mathbb{E}_{W_k \\sim \\bar{N}_t}[(1-w_t)\\times \\log(1 - \\mathcal{D}(Z_t, Z_k)) + w_t\\times \\log\\mathcal{D}(Z_t, Z_k)] \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba3d85",
   "metadata": {},
   "source": [
    "## 2.2. Implementacja kodera\n",
    "\n",
    "Ze względu na sekwencyjny charakter próbek w oknie, jako model kodera użyjemy dwukierunkowej sieci rekurencyjnego typu GRU, która na wyjściu cechy przekształci dodatkową warstwą liniową. Sieć rekurencyjna dostarcza nam stany ukryte w formie wektorów dla każdego punktu wejściowego w sekwencji, więc w celu otrzymania jednego wektora podsumowującego całą sekwencję należy dokonać odpowiedniej agragacji. W niniejszym koderze rozwarzymy dwie metody agregacji sekwencji stanów ukrytych:\n",
    "\n",
    "- wybór ostatniego stanu ukrytego\n",
    "- agregacja stanów ukrytych z wykorzystaniem mechanizmu uwagi (ang. _attention_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5757f-36f9-49d7-844d-741729a03fd5",
   "metadata": {},
   "source": [
    "## Zadanie 2.1 (1.5 pkt)\n",
    "\n",
    "Zaimplementuj dwie klasy dla dwóch niżej zdefiniowanych stanów ukrytych:\n",
    "\n",
    "1. `LastHiddenAggreagation` (0.5 pkt) - metoda `forward(...)` zwraca ostatni stan ukryty z sekwencji\n",
    "2. `AttentionAggregation` (1 pkt):\n",
    "   - aplikuje [_scaled dot product attention_](https://arxiv.org/abs/1706.03762) oraz oblicza średnią z otrzymanych wektorów\n",
    "   - podpowiedź: możesz wykorzystać `torch.nn.MultiHeadAttention` z parametrem `num_heads=1`, ale **należy mieć zrozumienie i intuicje, jak działa mechanizm uwagi i jak może pomagać w agregacji**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57feb3-1825-46a9-b2a0-70178a9d5b60",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "243bd54689b1baf268154b7b9818a1f3",
     "grade": true,
     "grade_id": "aggregation-implementation",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LastHiddenAggregation(nn.Module):\n",
    "    def forward(self, rnn_output: Tensor) -> Tensor:\n",
    "        \"\"\"Aggregates information from sequence of vectors by taking last state.\n",
    "        rnn_output: Tensor with dimensions [sequence_length, batch_size, hidden_dim]\n",
    "        returns: Tensor with dimensions [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class AttentionAggregation(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, rnn_output: Tensor) -> Tensor:\n",
    "        \"\"\"Aggregates information from sequence of vectors with additive attention.\n",
    "        rnn_output: Tensor with dimensions [sequence_length, batch_size, hidden_dim]\n",
    "        returns: Tensor with dimensions [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce9918",
   "metadata": {},
   "source": [
    "## Zadanie 2.2 (1.5 pkt)\n",
    "\n",
    "Zaimplementuj klasę `GRUEncoder` zgodnie z poniższymi wymaganiami:\n",
    "\n",
    "- w metodzie `__init__()`:\n",
    "  - utwórz instancję sieci GRU o wymiarze wejściowym `in_dim`, wymiarze ukrytym `hidden_dim`, która posiada jedną warstwę oraz jest dwukierunkowa (ustaw dodatkowo parametr `batch_first=False`)\n",
    "  - utwórz instancję warstwy liniowej o odpowiednim wymiarze wejściowym oraz wyjściu o wymiarowości `emb_dim`\n",
    "- w metodzie `_get_initial_hidden()` utwórz wektor zer jako początkowy stan ukryty\n",
    "- w metodzie `forward()` przetwórz cechy okień podanych na wejściu i wyznacz wektory reprezentacji (używając sieci GRU a następnie warstwy liniowej)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5f3fc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e3d96abdecebf8f56be26f0dd3c7fb4",
     "grade": true,
     "grade_id": "encoder-implementation",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agg: Literal[\"last\", \"attn\"],\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        emb_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        if agg == \"last\":\n",
    "            self.agg = LastHiddenAggregation()\n",
    "        elif agg == \"attn\":\n",
    "            self.agg = AttentionAggregation(2 * hidden_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation name\")\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_initial_hidden(self, batch_size):\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        emb = ...\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb755b4e",
   "metadata": {},
   "source": [
    "## 2.3. Implementacja dyskryminatora\n",
    "\n",
    "Jako dyskryminatora użyjemy prostego dwuwarstwowego perceptrona, tak aby nie ryzykować przeuczeniem tego komponentu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba8e73",
   "metadata": {},
   "source": [
    "## Zadanie 2.3 (2 pkt)\n",
    "\n",
    "Zaimplementuj 2 modele dyskryminatora zgodnie z następującymi wymaganiami:\n",
    "\n",
    "`DotProductDiscriminator` (1 pkt):\n",
    "\n",
    "- w metodzie `forward()` zwróc iloczyn skalarny odpowiadających sobie reprezentacji (uprzednio znormalizuj reprezentacje do jednostkowej długości)\n",
    "\n",
    "`MLPDiscriminator` (1 pkt):\n",
    "\n",
    "- w metodzie `__init__()` utwórz wielowarstwowy perceptron z następującymi warstwami (`d` to wymiar wejściowy):\n",
    "  - warstwa liniowa o rozmiarach 2*d na 4*d\n",
    "  - aktywacja ReLU\n",
    "  - dropout z prawdp. równym 0.5\n",
    "  - kolejna warstwa liniowa o rozmiarze 4\\*d na 1\n",
    "- w metodzie `forward()` dokonaj konkatenacji reprezentacji `z` oraz `z_tilde` a następnie przekaż je do perceptrona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae37ea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9554ceb62372dc86dcbceb81c6f47bad",
     "grade": true,
     "grade_id": "discriminator-implementation",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DotProductDiscriminator(nn.Module):\n",
    "    def forward(self, z: Tensor, z_tilde: Tensor) -> Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MLPDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, z: Tensor, z_tilde: Tensor) -> Tensor:\n",
    "        p = ...\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return p.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb316d",
   "metadata": {},
   "source": [
    "## 2.4. Implementacja funkcji kosztu\n",
    "\n",
    "W celu implementacji funkcji kosztu możemy wspomóc się gotową funkcją binarnej entropii krzyżowej, zauważając że problem uczenia dyskryminatora (a zatem całego modelu TNC) sprowadza się do problemu klasyfikacji binarnej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43449f6",
   "metadata": {},
   "source": [
    "## Zadanie 2.4 (3 pkt)\n",
    "\n",
    "Zaimplementuj moduł implementujący funkcję kosztu modelu TNC stosując następujące wskazówki/instrukcje:\n",
    "\n",
    "- `z_t` to reprezentacje obecnie rozważanych okien, `z_p` to reprezentacje pozytywne, natomiast `z_n` to reprezentacje nieoznaczone\n",
    "- najpierw wyznacz predykcje dyskryminatora dla pary `(z_t, z_p)` oraz `(z_t, z_n)`\n",
    "- przygotuj etykiety (zera i jedynki) dla par podobnych oraz różnych\n",
    "- oblicz funkcję binarnej entropii krzyżowej dla wyników dyskryminatora, pamiętaj, że:\n",
    "  - parę `(z_t, z_p)` chcemy traktować jako pozytywną (klasa 1)\n",
    "  - parę `(z_t, z_n)` chcemy traktować jako pozytywną z wagą `w` oraz jako negatywną (klasa 0) z wagą `1 - w`\n",
    "- zaimplementuj metodę `_compute_accuracy()`, która sprawdzi czy decyzje dyskryminatora są właściwe (pary pozytywne mają prawdp. `> 0.5`, a pary negatwne `< 0.5`; pamiętaj o zastosowaniu funkcji sigmoid!); wyniki uśrednij uzyskując miarę accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc611e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06cc1675bf94eb195e3777d2d9d1c186",
     "grade": true,
     "grade_id": "loss-function-implementation",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class TNCLossFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, discriminator: Literal[\"dot\", \"mlp\"], emb_dim: int, w: float):\n",
    "        super().__init__()\n",
    "\n",
    "        if discriminator == \"dot\":\n",
    "            self.discriminator = DotProductDiscriminator()\n",
    "        elif discriminator == \"mlp\":\n",
    "            self.discriminator = MLPDiscriminator(input_dim=emb_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid discriminator: {discriminator}\")\n",
    "\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss()\n",
    "        self.w = w\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z_t: Tensor,\n",
    "        z_p: Tensor,\n",
    "        z_n: Tensor,\n",
    "    ) -> Tuple[Tensor, float]:\n",
    "        d_p = ...\n",
    "        d_n = ...\n",
    "        loss = ...\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        accuracy = self._compute_accuracy(d_p=d_p, d_n=d_n)\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_accuracy(d_p: Tensor, d_n: Tensor) -> float:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d31cf1",
   "metadata": {},
   "source": [
    "## 2.5. Uruchomienie modelu TNC\n",
    "\n",
    "Poniżej znajduje się kod pozwalający wyuczyć model TNC (implementacja w bibliotece PyTorch-Lightning). Ustawimy domyślny zbiór hiperparametrów i będziemy uczyć model przez 50 epok. Następnie zwizualizujemy otrzymane wektory reprezentacji i zastosujemy je w zadaniu klasyfikacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351605e8",
   "metadata": {},
   "source": [
    "Moduł danych:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a472ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import TNCDataset\n",
    "\n",
    "\n",
    "class TrainDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mc_sample_size: int,\n",
    "        window_size: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mc_sample_size = mc_sample_size\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = torch.load(f=\"./data/simulated.pt\")\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"x_train\")\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"x_val\", shuffle=False)\n",
    "\n",
    "    def _dataloader(self, split: str, shuffle: bool = True) -> DataLoader:\n",
    "        data = TNCDataset(\n",
    "            x=self.dataset[split],\n",
    "            mc_sample_size=self.mc_sample_size,\n",
    "            window_size=self.window_size,\n",
    "        )\n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=int(os.environ.get(\"NUM_WORKERS\", 0)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13820d",
   "metadata": {},
   "source": [
    "Model TNC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class TNCModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams: dict[str, Any]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.encoder = GRUEncoder(\n",
    "            agg=hparams[\"agg\"],\n",
    "            in_dim=hparams[\"in_dim\"],\n",
    "            hidden_dim=hparams[\"hidden_dim\"],\n",
    "            emb_dim=hparams[\"emb_dim\"],\n",
    "        )\n",
    "        self._loss_fn = TNCLossFunction(\n",
    "            discriminator=hparams[\"discriminator\"],\n",
    "            emb_dim=self.encoder.emb_dim,\n",
    "            w=self.hparams[\"w\"],\n",
    "        )\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self._common_step(batch)\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._common_step(batch)\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx) -> None:\n",
    "        self.training_step_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx) -> None:\n",
    "        self.validation_step_outputs.append(outputs)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        avg_loss, avg_accs = self._summarize_outputs(self.training_step_outputs)\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\"train/acc\", avg_accs, on_epoch=True, on_step=False)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss, avg_accs = self._summarize_outputs(self.validation_step_outputs)\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"val/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\"val/acc\", avg_accs, on_epoch=True, on_step=False)\n",
    "\n",
    "    def _common_step(self, batch):\n",
    "        x_t, x_p, x_n, _ = batch\n",
    "        mc_sample = x_p.shape[1]\n",
    "        batch_size, f_size, len_size = x_t.shape\n",
    "\n",
    "        x_p = x_p.reshape((-1, f_size, len_size))\n",
    "        x_n = x_n.reshape((-1, f_size, len_size))\n",
    "        x_t = x_t.repeat(mc_sample, 1, 1)\n",
    "\n",
    "        z_t = self.encoder(x_t)\n",
    "        z_p = self.encoder(x_p)\n",
    "        z_n = self.encoder(x_n)\n",
    "\n",
    "        loss, acc = self._loss_fn(z_t=z_t, z_p=z_p, z_n=z_n)\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _summarize_outputs(outputs):\n",
    "        losses = [out[\"loss\"].item() for out in outputs]\n",
    "        accs = [out[\"acc\"] for out in outputs]\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_accs = np.mean(accs)\n",
    "\n",
    "        return avg_loss, avg_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b9544",
   "metadata": {},
   "source": [
    "Ustawienie domyślnych hiperparametrów:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hparams = {\n",
    "    \"agg\": \"last\",\n",
    "    \"discriminator\": \"dot\",\n",
    "    \"in_dim\": 3,\n",
    "    \"hidden_dim\": 100,\n",
    "    \"emb_dim\": 10,\n",
    "    \"window_size\": 50,\n",
    "    \"mc_sample_size\": 40,\n",
    "    \"w\": 0.05,\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"batch_size\": 10,\n",
    "    \"name\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31caa9d1",
   "metadata": {},
   "source": [
    "Uczenie modelu TNC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b89be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bc9f8-69c6-45d9-b787-80e8cad12ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = TrainDataModule(\n",
    "    mc_sample_size=default_hparams[\"mc_sample_size\"],\n",
    "    window_size=default_hparams[\"window_size\"],\n",
    "    batch_size=default_hparams[\"batch_size\"],\n",
    ")\n",
    "datamodule.setup(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f39ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "def train(hparams):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    datamodule = TrainDataModule(\n",
    "        mc_sample_size=hparams[\"mc_sample_size\"],\n",
    "        window_size=hparams[\"window_size\"],\n",
    "        batch_size=hparams[\"batch_size\"],\n",
    "    )\n",
    "    tnc = TNCModel(hparams)\n",
    "\n",
    "    model_chkpt = ModelCheckpoint(\n",
    "        dirpath=f\"./data/checkpoints/{hparams['name']}/\",\n",
    "        filename=\"model\",\n",
    "        monitor=\"val/acc\",\n",
    "        mode=\"max\",\n",
    "        verbose=False,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=\"./data/logs\",\n",
    "            name=f\"TNC_{hparams['name']}\",\n",
    "            default_hp_metric=False,\n",
    "        ),\n",
    "        callbacks=[model_chkpt],\n",
    "        num_sanity_val_steps=0,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=hparams[\"num_epochs\"],\n",
    "        accelerator=\"cpu\",  # change to \"cuda\", if want to train on GPU\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=tnc, datamodule=datamodule)\n",
    "\n",
    "\n",
    "train(hparams=default_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca1911",
   "metadata": {},
   "source": [
    "Ewaluacja modelu:\n",
    "\n",
    "- wizualizacja wektorów ukrytych i surowych danych\n",
    "- zadanie klasyfikacji stanu procesu generującego ciąg czasowy\n",
    "- zadanie klasteryzacji wektorów reprezentacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3262d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import visualize_embeddings\n",
    "from src.evaluations import evaluate_model\n",
    "\n",
    "\n",
    "def visualize(name: str):\n",
    "    best_tnc_model = TNCModel.load_from_checkpoint(\n",
    "        checkpoint_path=f\"./data/checkpoints/{name}/model.ckpt\"\n",
    "    )\n",
    "\n",
    "    best_encoder = best_tnc_model.encoder\n",
    "\n",
    "    dataset = torch.load(\"./data/simulated.pt\")\n",
    "\n",
    "    fig = visualize_embeddings(\n",
    "        x_all=dataset[\"x_all\"],\n",
    "        y_all=dataset[\"y_all\"],\n",
    "        encoder=best_encoder,\n",
    "        window_size=best_tnc_model.hparams[\"window_size\"],\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "visualize(default_hparams[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cb906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_clustering(name: str):\n",
    "    best_tnc_model = TNCModel.load_from_checkpoint(\n",
    "        checkpoint_path=f\"./data/checkpoints/{name}/model.ckpt\"\n",
    "    )\n",
    "\n",
    "    best_encoder = best_tnc_model.encoder\n",
    "\n",
    "    dataset = torch.load(\"./data/simulated.pt\")\n",
    "\n",
    "    metrics = evaluate_model(\n",
    "        dataset=dataset,\n",
    "        encoder=best_encoder,\n",
    "        window_size=best_tnc_model.hparams[\"window_size\"],\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "evaluate_classification_clustering(default_hparams[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6640a-47b8-4490-bc1e-0f37ee83ee2c",
   "metadata": {},
   "source": [
    "## Zadanie 2.5. Badanie wpływu metody agregacji oraz dyskryminatora TNC (1.5 pkt)\n",
    "\n",
    "Korzystając z domyślnych hiperparametrów sprawdź, który z zaimplementowanych modułów agregacji oraz dyskryminatora dostarcza najlepszych rezultatów. Sprawdź wszystkie 4 kombinacje $\\{\\mathrm{last}, \\mathrm{attn}\\} \\times \\{\\mathrm{dot}, \\mathrm{mlp}\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae042378-f7d8-497f-9516-9ee86bd3bca5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c8d36cf334b53947f87c72199bce399",
     "grade": true,
     "grade_id": "module-study",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86d3c7",
   "metadata": {},
   "source": [
    "## Zadanie 2.6. Badanie hiperparametrów metody TNC (1.5 pkt)\n",
    "\n",
    "Korzystając z podanych funkcji `train()` oraz `evaluate_classification_clustering()` zbadaj następujące hiperparametry:\n",
    "\n",
    "- `window_size` (zbadaj co najmniej 3 wartości)\n",
    "- `w` (zbadaj co najmniej 3 wartości)\n",
    "\n",
    "Jeśli uczenie modelu będzie zbyt czasochłonne można zredukować liczbę epok. Każdy z parametrów można zbadać osobno - nie ma potrzeby przeglądania ich przekroju. Analizy powinny być prowadzone na podstawie metryk: AUC (zbiór testowy) oraz metryki klasteryzacji (Silhouette oraz Davies-Bouldin). Utwórz tabelki, które podsumują wyniki eksperymentów. Skomentuj wyniki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dafbf7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f022448639ccb1a491238800bf03adf2",
     "grade": true,
     "grade_id": "hyperparameter-study",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
